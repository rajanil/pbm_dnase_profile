\documentclass[reqno]{amsart}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
%\usepackage{stmaryrd}
%\usepackage{mathrsfs}
%\usepackage{pifont}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage[usenames,dvipsnames]{color}
\numberwithin{equation}{section}
\usepackage{url}
\usepackage{calc}

\oddsidemargin = 0.3in
\evensidemargin = 0.3in
\textwidth = 6in

\input{aliases}

\begin{document}

\title{CENTIPEDE 2.0}

\author{Anil Raj}
\author{Heejung Shim}

\date{\today}

\maketitle

\section{Overview}
CENTIPEDE aims to infer motif sites bound by transcription factors based on the DNase I cleavage patterns
measured from DNase-Seq assays. The model relies on two assumptions: (1) sites bound by transcription
factors have higher DNase I sensitivity than unbound sites, and (2) each transcription factor has a 
characteristic DNase I cleavage profile at bound sites.

Given a putative binding site, CENTIPEDE models the number of reads mapped to each base pair as a mixture
of two distributions, where the mixing proportions capture the probability of the factor being bound.
Specifically, conditional on being bound, the total number of reads are modelled as drawn from a 
negative binomial distribution and the read profile, conditional on the total number of reads, are 
modelled as drawn from a multinomial distribution.

\begin{align}
    p(X_n, T^X_n | Z_n=1) 
        & = p(X_n | Z_n=1, T^X_n) p(T^X_n | Z_n=1) \\
        & = \multinomial(\pi; T^X_n) \negbinomial(\alpha, \tau) \\
    p(X_n, T^X_n | Z_n=0)
        & = p(X_n | Z_n=0, T^X_n) p(T^X_n | Z_n=0) \\
        & = \multinomial(\pi_o; T^X_n) \negbinomial(\alpha_o, \tau_o),
\end{align}
where $X_n \in \mathbb{N}_0^L$, $T^X_n = \sum_l X_{nl}$, $\pi \in \mathbb{S}^L$, $\mathbb{S}^L$ is the standard $L$-simplex, $\pi_o = \frac{1}{L}\mathbf{1}^L$, $\mathbf{1}^L$ is the $L$-dimensional vector of ones, $\alpha, \alpha_o \in \mathbb{R}^+$, and $\tau, \tau_o \in [0,1]$. $L$ is the length of the site around the binding motif.

A key limitation of this generative model is that it does not appropriately model the underlying spatially 
structured DNase I cleavage pattern induced by the binding of a particular transcription factor. 
%One way to model spatially structured signals would be to place 
%a logistic-normal prior on the multinomial parameter $\pi$; the covariance matrix in the logistic-normal 
%distribution can then be estimated from the data. However, the non-conjugacy of this prior makes exact 
%inference intractable, leading us to use approximate techniques like variational inference to compute the 
%posterior probabilities of the latent variables \anil{This approach will be derived and discussed later}.
We propose to replace the multinomial part of the CENTIPEDE model with a hierarchical multi-scale poisson model. 
Here, we briefly describe the model and derive maximum likelihood estimators for parameters. 
See Kolaczyk (1999) for a more detailed discussion of this inhomogenous multi-scale poisson process model. 

\section{Poisson-Binomial Model}
Specifically, keeping Kolaczyk's notation for the parameters, let $Y_{jk}$ be defined in terms of the data $X_l$ as
\begin{align}
    Y_{J-1,k} &= X_{2k}+X_{2k+1} \\
    Y_{j,k} &= Y_{j+1,2k}+Y_{j+1,2k+1}, \qquad j \in \{0,\ldots,J-2\},
\end{align}
where $j \in \{0,\ldots,J-1\}$, $J=\logtwo{L}$, and, $k \in \{0,\ldots,2^{j}-1\}$. Note that, the
range of the $k$ index depends on the value of the $j$ index. Conditional on the total number of reads 
at a site, the likelihood function of parameter $R = (R_{jk})$ factorizes as follows:
\begin{align}
    p(X_n | Z_n=1, T^X_n, R) = \prod_{j,k} p(Y_{njk} | Z_n=1, T_{njk}, R_{jk}),
\end{align}
where
\begin{align}
    R_{jk} &= \gamma_{jk} \diracdelta{\half} + (1 - \gamma_{jk}) \diracdelta{B_{jk}} \\
    p(\gamma_{jk}) &= \bernoulli(\pi_j), \\
\end{align}
and, $\diracdelta{a}$ is the Dirac delta function centered at $a$. For each scale $j$, the product is only 
over even values of the index $k$. For ease of notation, we introduce a variable $T_{njk} = Y_{n,j-1,k/2}$, the 
total number of reads at a coarser resolution from which reads at the finer resolution are
drawn. Therefore, at a given scale and location, $Y_{njk}$ is effectively drawn from a mixture of
a binomial distribution, with parameter $\half$ and a binomial distribution with parameter $B_{jk}$.
If $R_{jk}$ were treated as a fixed parameter instead of a random variable, we get back the
original multinomial model in CENTIPEDE.

We will derive and explore the following three variations of the above model. 
\begin{enumerate}
    \item $B_{jk}$ is a fixed parameter to be estimated by maximizing the likelihood of the model.
    \item $B_{jk}$ is a latent random variable, $B_{jk} \sim \betadist(\mu_j, \mu_j)$.
    \item To account for overdispersion across sites, we allow for site-specific values for $R$, drawn from a beta
    distribution with scale-dependent mean and scale- and location-dependent variances.
\end{enumerate}
Specifically, the three variations are: 
\begin{enumerate}
    \item $\begin{aligned}[t]
        Y_{njk} | T_{njk}, R_{jk} &\sim \binomial(R_{jk}; T_{njk}) \\
        R_{jk} &= \gamma_{jk} \diracdelta{\half} + (1 - \gamma_{jk}) \diracdelta{B_{jk}} \\
        \gamma_{jk} &\sim \bernoulli(\pi_j) \\
        \mathrm{Thus,} \quad Y_{njk} | T_{njk}, \pi_j, B_{jk} &\sim \pi \binomial(\half; T_{njk}) + (1-\pi) \binomial(B_{jk}; T_{njk})
    \end{aligned}$
    \item $\begin{aligned}[t]
        Y_{njk} | T_{njk}, R_{jk} &\sim \binomial(R_{jk}; T_{njk}) \\
        R_{jk} &= \gamma_{jk} \diracdelta{\half} + (1 - \gamma_{jk}) \diracdelta{B_{jk}} \\
        \gamma_{jk} &\sim \bernoulli(\pi_j) \\
        B_{jk} &\sim \betadist(\mu_j, \mu_j) \\
        \mathrm{Thus,} \quad Y_{njk} | T_{njk}, \pi_j, B_{jk} &\sim \pi \binomial(\half; T_{njk}) + (1-\pi) \betabinomial(\mu_j, \mu_j; T_{njk})
    \end{aligned}$
    \item $\begin{aligned}[t]
        Y_{njk} | T_{njk}, R_{njk} &\sim \binomial(R_{njk}; T_{njk}) \\
        R_{njk} &\sim \betadist(R_{jk} \tau_{jk}, (1-R_{jk}) \tau_{jk}) \\
        R_{jk} &= \gamma_{jk} \diracdelta{\half} + (1 - \gamma_{jk}) B_{jk} \\
        \gamma_{jk} &\sim \bernoulli(\pi_j) \\
        \mathrm{Thus,} \quad Y_{njk} | T_{njk}, \pi_j, B_{jk} &\sim \pi \betabinomial(\half \tau_{jk}, \half \tau_{jk}; T_{njk}) \\
            &+ (1-\pi) \betabinomial(B_{jk}\tau_{jk}, (1-B_{jk})\tau_{jk}; T_{njk})
    \end{aligned}$
\end{enumerate}

In the framework of CENTIPEDE, we now have two sets of latent variables: one specifying whether a factor
is bound at a site or not, and the other specifying the degree of smoothness in DNase I cleavage rates
at different scales and locations around a specific motif. 
Let us assume that the latent variable $Z_n$ is observed to be $1$ for a set of sites. 
Maximum likelihood estimates for the parameters in each model can be calculated by 
maximizing the lower bound on the likelihood, obtained
by proposing a family of posterior distributions $q(\gamma_{jk})$, using the EM algorithm.
\begin{align}
    q(\gamma_{jk}) = \bernoulli(\gammav_{jk})
\end{align}

\subsection{Model 1}

For this model, the lower bound to the log-likelihood can be derived as follows:

\begin{align}
    \likelihood
        &= \sum_{n,j,k} \log p(Y_{njk} | \pi_j, B_{jk}; T_{\cdot jk}) \\
        &= \sum_{n,j,k} \log \sum_{\gamma_{jk}} p(Y_{njk} | \gamma_{jk}, B_{jk}; T_{njk}) p(\gamma_{jk} | \pi_j) \\
        &\geq \sum_{n,j,k} \sum_{\gamma_{jk}} q(\gamma_{jk}) \left( \log p(Y_{njk} | \gamma_{jk}, B_{jk}; T_{njk})
        + \log \frac{p(\gamma_{jk} | \pi_j)}{q(\gamma_{jk})} \right) \\
        &= \sum_{n,j,k} \expect{\log p(Y_{njk} | \gamma_{jk}; T_{njk})}{q(\gamma)}
        + \expect{\log \frac{p(\gamma_{jk} | \pi_j)}{q(\gamma_{jk})}}{q(\gamma)} \\
        &= \sum_{n,j,k} \expect{\gamma_{jk} \log p(Y_{njk} | \half; T_{njk}) + (1-\gamma_{jk}) \log p(Y_{njk} | B_{jk}; T_{njk})}{q(\gamma)}
        + \expect{\log \frac{p(\gamma_{jk} | \pi_j)}{q(\gamma_{jk})}}{q(\gamma)} \\
        &= \sum_{n,j,k} \gammav_{jk} \log p(Y_{njk} | \half; T_{njk}) + (1-\gammav_{jk}) \log p(Y_{njk} | B_{jk}; T_{njk})
        + \expect{\log \frac{p(\gamma_{jk} | \pi_j)}{q(\gamma_{jk})}}{q(\gamma)}.
\end{align}

The distributions in the first and second terms are binomial distributions and the relevant likelihood functions can be written as follows.
\begin{align}
    \likelihood^b_{njk} 
        &= \log p(Y_{njk} | \half; T_{njk}) 
        = \mathcal{C}_{njk} + T_{njk} \log \left( \half \right)\\
    \likelihood^{bb}_{njk}(B_{jk}) 
        &= \log p(Y_{njk} | B_{jk}; T_{njk})
        = \mathcal{C}_{njk} + Y_{njk} \log(B_{jk}) + (T_{njk}-Y_{njk}) \log(1-B_{jk})
\end{align}
Thus,
\begin{align}
    \likelihood
        &= \sum_{n,j,k} \gammav_{jk} \likelihood^b_{njk} + (1-\gammav_{jk}) \likelihood^{bb}_{njk}(B_{jk})
        + \gammav_{jk} \log \frac{\pi_j}{\gammav_{jk}} + (1-\gammav_{jk}) \log \frac{(1-\pi_j)}{(1-\gammav_{jk})}
\end{align}

Maximizing $\likelihood$ with respect to $\gammav_{jk}$ while keeping other parameters fixed gives
\begin{align}
    \frac{\partial \likelihood}{\partial \gammav_{jk}}
        &= \frac{1}{N} \sum_n \likelihood^b_{njk} - \likelihood^{bb}_{njk}(B_{jk}) + \log \pi_j - \log (1-\pi_j) - \log \gammav_{jk} + \log (1-\gammav_{jk}) = 0
\end{align}
\begin{align}
    \log \frac{\gammav_{jk}}{1-\gammav_{jk}}
        &= \log \frac{\pi_j}{1-\pi_j} + \frac{1}{N} \sum_n \likelihood^b_{njk} - \likelihood^{bb}_{njk}(B_{jk})
\end{align}

Maximizing $\likelihood$ with respect to $\pi_j$ while keeping other parameters fixed gives
\begin{align}
    \frac{\partial \likelihood}{\partial \pi_j}
        &= \frac{1}{\pi_j} \sum_k \gammav_{jk} - \frac{1}{1-\pi_j} \sum_k (1-\gammav_{jk}) = 0
\end{align}
\begin{align}
    \pi_j = \frac{1}{K} \sum_k \gammav_{jk}
\end{align}

Maximizing $\likelihood$ with respect to $B_{jk}$ while keeping other parameters fixed gives
\begin{align}
    \frac{\partial \likelihood}{\partial B_{jk}}
        &= \frac{1-\gamma_{jk}}{B_{jk}} \sum_n Y_{njk} - \frac{(1-\gamma_{jk})}{1-B_{jk}} \sum_n (T_{njk}-Y_{njk}) = 0
\end{align}
\begin{align}
    B_{jk} = \frac{\sum_n Y_{njk}}{\sum_n T_{njk}}
\end{align}

\subsection{Model 2}

Following the derivation in the previous model, the lower bound to the log-likelihood can be written as follows:
\begin{align}
    \likelihood
        &\geq \sum_{n,j,k} \gammav_{jk} \log p(Y_{njk} | \half; T_{njk}) + (1-\gammav_{jk}) \log p(Y_{njk} | \mu_j; T_{njk})
        + \expect{\log \frac{p(\gamma_{jk} | \pi_j)}{q(\gamma_{jk})}}{q(\gamma)}.
\end{align}

The distribution in the first term is a binomial distribution and the distribution in the second is a ``symmetric'' beta-binomial distribution.
Since the parameters of these distributions are fixed, we will replace the relevant likelihood functions as follows.
\begin{align}
    \likelihood^b_{njk} &= \log p(Y_{njk} | \half; T_{njk}) \\
    \likelihood^{bb}_{njk} &= \log p(Y_{njk} | \mu_j; T_{njk})
\end{align}
Thus,
\begin{align}
    \likelihood
        &= \sum_{n,j,k} \gammav_{jk} \likelihood^b_{njk} + (1-\gammav_{jk}) \likelihood^{bb}_{njk}
        + \gammav_{jk} \log \frac{\pi_j}{\gammav_{jk}} + (1-\gammav_{jk}) \log \frac{(1-\pi_j)}{(1-\gammav_{jk})}
\end{align}

Maximum likelihood estimates for $\gamma_{jk}$ and $\pi_j$, as derived earlier, can be written as
\begin{align}
    \log \frac{\gammav_{jk}}{1-\gammav_{jk}}
        &= \log \frac{\pi_j}{1-\pi_j} + \frac{1}{N} \sum_n \likelihood^b_{njk} - \likelihood^{bb}_{njk} \\
    \pi_j 
        &= \frac{1}{K} \sum_k \gammav_{jk}
\end{align}

\subsection{Model 3}

Again, as before, the lower bound to the log-likelihood can be written as follows:
\begin{align}
    \likelihood
        &\geq \sum_{n,j,k} \gammav_{jk} \log p(Y_{njk} | \half, \tau_{jk}; T_{njk}) + (1-\gammav_{jk}) \log p(Y_{njk} | B_{jk}, \tau_{jk}; T_{njk})
        + \expect{\log \frac{p(\gamma_{jk} | \pi_j)}{q(\gamma_{jk})}}{q(\gamma)}.
\end{align}

The distributions in the first and second terms are beta-binomial distributions and the relevant likelihood functions can be written as follows.
\begin{align}
    \likelihood^b_{njk}(\tau_{jk})
        &= \log p(Y_{njk} | \half, \tau_{jk}; T_{njk})
        = \mathcal{C}_{njk} + \gammaln(Y_{njk}+0.5\tau_{jk}) + \gammaln(T_{njk}-Y_{njk}+0.5\tau_{jk}) \notag\\
        &- \gammaln(T_{njk}+\tau_{jk}) + \gammaln(\tau_{jk}) - 2*\gammaln(0.5\tau_{jk}) \\
    \likelihood^{bb}_{njk}(B_{jk},\tau_{jk})
        &= \log p(Y_{njk} | B_{jk}; T_{njk})
        = \mathcal{C}_{njk} + \gammaln(Y_{njk}+B_{jk}\tau_{jk}) + \gammaln(T_{njk}-Y_{njk}+(1-B_{jk})\tau_{jk}) \notag\\
        &- \gammaln(T_{njk}+\tau_{jk}) + \gammaln(\tau_{jk}) - \gammaln(B_{jk}\tau_{jk}) - \gammaln((1-B_{jk})\tau_{jk})
\end{align}

Thus,
\begin{align}
    \likelihood
        &= \sum_{n,j,k} \gammav_{jk} \likelihood^b_{njk}(\tau_{jk}) + (1-\gammav_{jk}) \likelihood^{bb}_{njk}(B_{jk},\tau_{jk})
        + \gammav_{jk} \log \frac{\pi_j}{\gammav_{jk}} + (1-\gammav_{jk}) \log \frac{(1-\pi_j)}{(1-\gammav_{jk})}
\end{align}

Maximum likelihood estimates for $\gamma_{jk}$ and $\pi_j$ are the same as derived in the earlier two models. Since the remaining
parameters $B_{jk}$ and $\tau_{jk}$ occur within $\gammaln(\cdot)$ functions, closed form update equations
for these parameters cannot be derived. Instead, we'll maximize the likelihood with respect to these parameters using
generalized convex optimization algorithms. The gradient of the likelihood with respect to each of these parameters can be
derived as follows.

\begin{align}
    \frac{\partial \likelihood}{\partial B_{jk}}
        &= (1-\gammav_{jk}) \sum_n \left[ \tau_{jk} \digamma(Y_{njk}+B_{jk}\tau_{jk}) 
        - \tau_{jk} \digamma(T_{njk}-Y_{njk}+(1-B_{jk})\tau_{jk}) \right. \notag\\
        &- \left. \tau_{jk} \digamma(B_{jk}\tau_{jk}) + \tau_{jk} \digamma((1-B_{jk})\tau_{jk}) \right] \\
    \frac{\partial \likelihood}{\partial \tau_{jk}}
        &= \sum_n \gammav_{jk} \left[ 0.5 \digamma(Y_{njk}+0.5\tau_{jk}) + 0.5 \digamma(T_{njk}-Y_{njk}+0.5\tau_{jk}) \right. \notag\\
        &- \left. \digamma(T_{njk}+\tau_{jk}) + \digamma(\tau_{jk}) - \digamma(0.5\tau_{jk}) \right] \notag\\
        &+ (1-\gammav_{jk}) \left[ B_{jk} \digamma(Y_{njk}+B_{jk}\tau_{jk})
        + (1-B_{jk}) \digamma(T_{njk}-Y_{njk}+(1-B_{jk})\tau_{jk}) \right. \notag\\
        &- \left. \digamma(T_{njk}+\tau_{jk}) + \digamma(\tau_{jk}) 
        - B_{jk} \digamma(B_{jk}\tau_{jk}) - (1-B_{jk}) \digamma((1-B_{jk})\tau_{jk}) \right]
\end{align}

\subsection{Posterior distribution of $R_{jk}$}
  
The posterior distribution of $R_{jk}$ can be computed as follows.
\begin{align}
    p(R_{jk} | Y_{\cdot jk}, T_{\cdot jk})
        &= p(R_{jk},  \gamma_{jk} = 1 | Y_{\cdot jk}, T_{\cdot jk}) + p(R_{jk},  \gamma_{jk} = 0 | Y_{\cdot jk}, T_{\cdot jk}) \\
        &= p(R_{jk}  | \gamma_{jk} =1, Y_{\cdot jk}, T_{\cdot jk})p(\gamma_{jk} =1 | Y_{\cdot jk}, T_{\cdot jk}) \notag\\
        &+ p(R_{jk} | \gamma_{jk} =0, Y_{\cdot jk}, T_{\cdot jk})p(\gamma_{jk} =0 | Y_{\cdot jk}, T_{\cdot jk})
\end{align}
where 
\begin{align}
	p(\gamma_{jk} =0 | Y_{\cdot jk}, T_{\cdot jk}) 
	&= \frac{p(Y_{\cdot jk} | \gamma_{jk} =0, T_{\cdot jk})(1-\pi_j)}{p(Y_{\cdot jk} | \gamma_{jk} =1, T_{\cdot jk})\pi_j  
    + p(Y_{\cdot jk} | \gamma_{jk} =0, T_{\cdot jk})(1-\pi_j)} \\
	&= 1- p(\gamma_{jk} =1 | Y_{\cdot jk}, T_{\cdot jk}). 
\end{align}
We will derive $p(Y_{\cdot jk} | \gamma_{jk} =0, T_{\cdot jk})$, $p(Y_{\cdot jk} | \gamma_{jk} =1, T_{\cdot jk})$, $p(R_{jk}  | \gamma_{jk} =0, Y_{\cdot jk}, T_{\cdot jk})$, and $p(R_{jk}  | \gamma_{jk} =1, Y_{\cdot jk}, T_{\cdot jk})$ for each model.


\subsubsection{Model 1}
\begin{align}
    p(Y_{\cdot jk} | \gamma_{jk} =0, T_{\cdot jk}) 
        &= [\prod_{n} {T_{njk} \choose Y_{njk}}] (B_{jk})^{\sum_{n}Y_{njk}} (1-B_{jk})^{\sum_{n}(T_{njk} - Y_{njk})}\\
    p(Y_{\cdot jk} | \gamma_{jk} =1, T_{\cdot jk})
       &= [\prod_{n} {T_{njk} \choose Y_{njk}}] (\frac{1}{2})^{\sum_{n}T_{njk}}\\
    p(R_{jk}  | \gamma_{jk} =0, Y_{\cdot jk}, T_{\cdot jk}) &=  \diracdelta{B_{jk}}\\
    p(R_{jk}  | \gamma_{jk} =1, Y_{\cdot jk}, T_{\cdot jk}) &=  \diracdelta{\frac{1}{2}}
\end{align}
Then, the posterior mean can be written as
\begin{align}
  \expect{R_{jk} | Y_{\cdot jk}, T_{\cdot jk}}{}
      &= \frac{(\frac{1}{2})^{(1+\sum_{n}T_{njk})} \pi_j 
        + (B_{jk})^{(1+\sum_{n}Y_{njk} )} (1-B_{jk})^{\sum_{n}(T_{njk} - Y_{njk})}(1-\pi_j)}{(\frac{1}{2})^{\sum_{n}T_{njk}} \pi_j 
        + (B_{jk})^{\sum_{n}Y_{njk}} (1-B_{jk})^{\sum_{n}(T_{njk} - Y_{njk})}(1-\pi_j) }.
\end{align}


\subsubsection{Model 2}
\begin{align}
    p(Y_{\cdot jk} | \gamma_{jk} =0, T_{\cdot jk}) 
       &= [\prod_{n} {T_{njk} \choose Y_{njk}}]  \frac{B(\mu_j + \sum_{n}Y_{njk} , \mu_j   
        +  \sum_{n}(T_{njk} - Y_{njk})     )}{B(\mu_j,\mu_j)}\\
    p(Y_{\cdot jk} | \gamma_{jk} =1, T_{\cdot jk})
       &= [\prod_{n} {T_{njk} \choose Y_{njk}}] (\frac{1}{2})^{\sum_{n}T_{njk}}\\
    p(R_{jk}  | \gamma_{jk} =0, Y_{\cdot jk}, T_{\cdot jk}) &=  \frac{\mu_j + \sum_{n}Y_{njk}}{2\mu_j +  \sum_{n}T_{njk}}\\
    p(R_{jk}  | \gamma_{jk} =1, Y_{\cdot jk}, T_{\cdot jk}) &=  \diracdelta{\frac{1}{2}}
\end{align}
Then, the posterior mean can be written as
\begin{align}
  \expect{R_{jk} | Y_{\cdot jk}, T_{\cdot jk}}{}
      &=  \frac{(\frac{1}{2})^{(1+\sum_{n}T_{njk})} \pi_j 
        +  \frac{B(1+\mu_j + \sum_{n}Y_{njk} , \mu_j   +  \sum_{n}(T_{njk} - Y_{njk}))}{B(\mu_j,\mu_j)}(1-\pi_j)}
        { (\frac{1}{2})^{\sum_{n}T_{njk}} \pi_j + \frac{B(\mu_j + \sum_{n}Y_{njk} , \mu_j   +  \sum_{n}(T_{njk} - Y_{njk}) )}
        {B(\mu_j,\mu_j)}(1-\pi_j)  }, 
\end{align}
where $B(\cdot,\cdot)$ is the beta function.

\subsubsection{Model 3}
\begin{align}
    p(Y_{\cdot jk} | \gamma_{jk} =0, T_{\cdot jk}) 
       &= \prod_{n} \int {T_{njk} \choose Y_{njk}} R_{njk}^{Y_{njk}} (1-R_{njk})^{T_{njk}-Y_{njk}} \frac{R_{njk}^{B_{jk}\tau_{jk} -1} (1-R_{njk})^{(1-B_{jk})\tau_{jk} -1}}{ B(B_{jk}\tau_{jk},(1-B_{jk})\tau_{jk})}   \mathrm{d}R_{njk} \\
       &=  \prod_{n} {T_{njk} \choose Y_{njk}} \frac{B(B_{jk}\tau_{jk} +  Y_{njk}   ,  (1-B_{jk})\tau_{jk} +   T_{njk}-Y_{njk} )} {B(B_{jk}\tau_{jk},(1-B_{jk})\tau_{jk})}\\
    p(Y_{\cdot jk} | \gamma_{jk} =1, T_{\cdot jk})
      &=  \prod_{n} {T_{njk} \choose Y_{njk}} \frac{B(\frac{1}{2}\tau_{jk} +  Y_{njk}   ,  \frac{1}{2}\tau_{jk} +   T_{njk}-Y_{njk} )} {B(\frac{1}{2}\tau_{jk},\frac{1}{2}\tau_{jk})}\\
    p(R_{jk}  | \gamma_{jk} =0, Y_{\cdot jk}, T_{\cdot jk}) &=  \diracdelta{B_{jk}}\\
    p(R_{jk}  | \gamma_{jk} =1, Y_{\cdot jk}, T_{\cdot jk}) &=  \diracdelta{\frac{1}{2}}
\end{align}
Then, the posterior mean can be written as
\begin{align}
      \expect{R_{jk} | Y_{\cdot jk}, T_{\cdot jk}}{}
      &=  \frac{  \frac{1}{2}\pi_j \prod_{n} \frac{B(\frac{1}{2}\tau_{jk} +  Y_{njk}   ,  \frac{1}{2}\tau_{jk} +   T_{njk}-Y_{njk} )} {B(\frac{1}{2}\tau_{jk},\frac{1}{2}\tau_{jk})}   + B_{jk}(1-\pi_j) \prod_{n} \frac{B(B_{jk}\tau_{jk} +  Y_{njk}   ,  (1-B_{jk})\tau_{jk} +   T_{njk}-Y_{njk} )} {B(B_{jk}\tau_{jk},(1-B_{jk})\tau_{jk})}}{  \pi_j \prod_{n} \frac{B(\frac{1}{2}\tau_{jk} +  Y_{njk}   ,  \frac{1}{2}\tau_{jk} +   T_{njk}-Y_{njk} )} {B(\frac{1}{2}\tau_{jk},\frac{1}{2}\tau_{jk})}   + (1-\pi_j) \prod_{n} \frac{B(B_{jk}\tau_{jk} +  Y_{njk}   ,  (1-B_{jk})\tau_{jk} +   T_{njk}-Y_{njk} )} {B(B_{jk}\tau_{jk},(1-B_{jk})\tau_{jk})}}
\end{align}


\section{Centipede-PBM}

Each of the above three models can be straightforwardly incorporated into CENTIPEDE's learning framework by first deriving 
how the Poisson-Binomial model modifies the likelihood function. The key change is restricted to the multinomial part
of CENTIPEDE's likelihood function. Here, we derive the form of the change for Model $1$, and then apply the change
directly to Models $2$ and $3$ 

\begin{align}
    \likelihood
        &= \sum_{n} \log \sum_{Z_n} p(X_n|Z_n,T^X_n) p(T^X_n|Z_n) p(Z_n|S_n,\beta) \\
        &\geq \sum_n \sum_{Z_n} q(Z_n) \log p(X_n|Z_n,T^X_n) + \sum_{Z_n} q(Z_n) \log p(T^X_n | Z_n) + \expect{\log \frac{p(Z_n)}{q(Z_n)}}{q(Z_n)} \\
        &= \sum_n \zv_n \log p(X_n|Z_n=1,T^X_n) + (1-\zv_n) \log p(X_n|Z_n=0,T^X_n) \notag\\
        &+ \sum_{Z_n} q(Z_n) \log p(T^X_n | Z_n) + \expect{\log \frac{p(Z_n)}{q(Z_n)}}{q(Z_n)} \\
        &= \sum_n \zv_n \log \left[ \prod_{j,k} \sum_{\gamma_{jk}} p(Y_{njk} | \gamma_{jk}, B_{jk}; T_{njk}) p(\gamma_{jk} | \pi_j) \right]
        + (1-\zv_n) \log \prod_{j,k} p(Y_{njk} | \half; T_{njk}) \notag\\
        &+ \sum_{Z_n} q(Z_n) \log p(T^X_n | Z_n) + \expect{\log \frac{p(Z_n)}{q(Z_n)}}{q(Z_n)} \\
        &\geq \sum_{n,j,k} \zv_n \sum_{\gamma_{jk}} q(\gamma_{jk}) \left( \log p(Y_{njk} | \gamma_{jk}, B_{jk}; T_{njk}) 
        + \log \frac{p(\gamma_{jk} | \pi_j)}{q(\gamma_{jk})} \right) 
        + (1-\zv_n) \log p(Y_{njk} | \half; T_{njk}) \notag\\
        &+ \sum_{Z_n} q(Z_n) \log p(T^X_n | Z_n) + \expect{\log \frac{p(Z_n)}{q(Z_n)}}{q(Z_n)} \\
        &= \sum_{n,j,k} \zv_n \left( \gammav_{jk} \log p(Y_{njk} | \half; T_{njk}) + (1-\gammav_{jk}) \log p(Y_{njk} | B_{jk}; T_{njk})
        + \expect{\log \frac{p(\gamma_{jk} | \pi_j)}{q(\gamma_{jk})}}{q(\gamma)} \right) \notag\\
        &+ (1-\zv_n) \log p(Y_{njk} | \half; T_{njk}) 
        + \sum_{Z_n} q(Z_n) \log p(T^X_n | Z_n) + \expect{\log \frac{p(Z_n)}{q(Z_n)}}{q(Z_n)} \\
        &= \sum_{n,j,k} \zv_n \left( \gammav_{jk} \likelihood^b_{njk} + (1-\gammav_{jk}) \likelihood^{bb}_{njk}(B_{jk})
        + \gammav_{jk} \log \frac{\pi_j}{\gammav_{jk}} + (1-\gammav_{jk}) \log \frac{(1-\pi_j)}{(1-\gammav_{jk})} \right) \notag\\
        &+ (1-\zv_n) \likelihood^b_{njk} + \sum_{Z_n} q(Z_n) \log p(T^X_n | Z_n) + \expect{\log \frac{p(Z_n)}{q(Z_n)}}{q(Z_n)} \\
\end{align}
The last two terms include the negative binomial contribution and the KL-divergence between the prior and posteriors, exactly as in CENTIPEDE.
Thus, the modified likelihood terms for the three models, and the relevant modified update equations, can be written as follows.

\subsection{Model 1}

The modified likelihood terms include
\begin{align}
    \likelihood
        &= \sum_{n,j,k} \zv_n \left( \gammav_{jk} \likelihood^b_{njk} + (1-\gammav_{jk}) \likelihood^{bb}_{njk}(B_{jk})
        + \gammav_{jk} \log \frac{\pi_j}{\gammav_{jk}} + (1-\gammav_{jk}) \log \frac{(1-\pi_j)}{(1-\gammav_{jk})} \right)
        + (1-\zv_n) \likelihood^b_{njk},
\end{align}
where
\begin{align}
    \likelihood^b_{njk}
        &= \log p(Y_{njk} | \half; T_{njk})
        = \mathcal{C}_{njk} + T_{njk} \log \left( \half \right)\\
    \likelihood^{bb}_{njk}(B_{jk})
        &= \log p(Y_{njk} | B_{jk}; T_{njk})
        = \mathcal{C}_{njk} + Y_{njk} \log(B_{jk}) + (T_{njk}-Y_{njk}) \log(1-B_{jk})
\end{align}
In the update equation for $\zv$, the likelihood functions of the multinomial parameters can be replaced by the following terms.
\begin{align}
    \log \frac{\zv_n}{1-\zv_n} 
        &= \sum_{j,k} (1-\gammav_{jk}) (\likelihood^{bb}_{njk}(B_{jk}) - \likelihood^b_{njk})
        + \gammav_{jk} \log \frac{\pi_j}{\gammav_{jk}} + (1-\gammav_{jk}) \log \frac{(1-\pi_j)}{(1-\gammav_{jk})}
        + \mathrm{remaining \, terms}
\end{align}
A similar modification applied for the other two models. The update equations for $\gammav$, $B$ and $\pi$ can be modified as follows:

\begin{align}
    \log \frac{\gammav_{jk}}{1-\gammav_{jk}}
        &= \log \frac{\pi_j}{1-\pi_j} + \frac{\sum_n \zv_n \left( \likelihood^b_{njk} - \likelihood^{bb}_{njk}(B_{jk}) \right)}{\sum_n \zv_n} \\
    \pi_j 
        &= \frac{1}{K} \sum_k \gammav_{jk} \\
    B_{jk} 
        &= \frac{\sum_n \zv_n Y_{njk}}{\sum_n \zv_n T_{njk}}
\end{align}

\subsection{Model 2}
\begin{align}
    \likelihood
        &= \sum_{n,j,k} \zv_n \left( \gammav_{jk} \likelihood^b_{njk} + (1-\gammav_{jk}) \likelihood^{bb}_{njk}
        + \gammav_{jk} \log \frac{\pi_j}{\gammav_{jk}} + (1-\gammav_{jk}) \log \frac{(1-\pi_j)}{(1-\gammav_{jk})} \right)
        + (1-\zv_n) \likelihood^b_{njk},
\end{align}
where
\begin{align}
    \likelihood^b_{njk} &= \log p(Y_{njk} | \half; T_{njk}) \\
    \likelihood^{bb}_{njk} &= \log p(Y_{njk} | \mu_j; T_{njk}).
\end{align}

Update equations for $\gammav$ and $\pi$ can be given as
\begin{align}
    \log \frac{\gammav_{jk}}{1-\gammav_{jk}}
        &= \log \frac{\pi_j}{1-\pi_j} + \frac{\sum_n \zv_n \left( \likelihood^b_{njk} - \likelihood^{bb}_{njk} \right)}{\sum_n \zv_n} \\
    \pi_j
        &= \frac{1}{K} \sum_k \gammav_{jk}
\end{align}

\subsection{Model 3}
\begin{align}
    \likelihood
        &= \sum_{n,j,k} \zv_n \left( \gammav_{jk} \likelihood^b_{njk}(\tau_{jk}) 
        + (1-\gammav_{jk}) \likelihood^{bb}_{njk}(B_{jk},\tau_{jk})
        + \gammav_{jk} \log \frac{\pi_j}{\gammav_{jk}} + (1-\gammav_{jk}) \log \frac{(1-\pi_j)}{(1-\gammav_{jk})} \right)
        + (1-\zv_n) \likelihood^o_{njk},
\end{align}
where
\begin{align}
    \likelihood^b_{njk}(\tau_{jk})
        &= \mathcal{C}_{njk} + \gammaln(Y_{njk}+0.5\tau_{jk}) + \gammaln(T_{njk}-Y_{njk}+0.5\tau_{jk}) \notag\\
        &- \gammaln(T_{njk}+\tau_{jk}) + \gammaln(\tau_{jk}) - 2*\gammaln(0.5\tau_{jk}) \\
    \likelihood^{bb}_{njk}(B_{jk},\tau_{jk})
        &= \mathcal{C}_{njk} + \gammaln(Y_{njk}+B_{jk}\tau_{jk}) + \gammaln(T_{njk}-Y_{njk}+(1-B_{jk})\tau_{jk}) \notag\\
        &- \gammaln(T_{njk}+\tau_{jk}) + \gammaln(\tau_{jk}) - \gammaln(B_{jk}\tau_{jk}) - \gammaln((1-B_{jk})\tau_{jk})\\
    \likelihood^o_{njk}(\tau^o_{jk})
        &= \mathcal{C}_{njk} + \gammaln(Y_{njk}+0.5\tau^o_{jk}) + \gammaln(T_{njk}-Y_{njk}+0.5\tau^o_{jk}) \notag\\
        &- \gammaln(T_{njk}+\tau^o_{jk}) + \gammaln(\tau^o_{jk}) - 2*\gammaln(0.5\tau^o_{jk})
\end{align}

Update equations for $\gammav$ and $\pi$ can be given as
\begin{align}
    \log \frac{\gammav_{jk}}{1-\gammav_{jk}}
        &= \log \frac{\pi_j}{1-\pi_j} + \frac{\sum_n \zv_n \left( \likelihood^b_{njk}(\tau_{jk}) 
        - \likelihood^{bb}_{njk}(B_{jk},\tau_{jk}) \right)}{\sum_n \zv_n} \\
    \pi_j
        &= \frac{1}{K} \sum_k \gammav_{jk}
\end{align}
The gradient of the likelihood with respect to $B$ and $\tau$ can be derived as follows.

\begin{align}
    \frac{\partial \likelihood}{\partial B_{jk}}
        &= \tau_{jk} (1-\gammav_{jk}) \sum_n \zv_n \left[ \digamma(Y_{njk}+B_{jk}\tau_{jk})
        - \digamma(T_{njk}-Y_{njk}+(1-B_{jk})\tau_{jk}) \right. \notag\\
        &- \left. \digamma(B_{jk}\tau_{jk}) + \digamma((1-B_{jk})\tau_{jk}) \right] \\
    \frac{\partial \likelihood}{\partial \tau_{jk}}
        &= \sum_n \zv_n \gammav_{jk} \left[ 0.5 \digamma(Y_{njk}+0.5\tau_{jk}) + 0.5 \digamma(T_{njk}-Y_{njk}+0.5\tau_{jk}) \right. \notag\\
        &- \left. \digamma(T_{njk}+\tau_{jk}) + \digamma(\tau_{jk}) - \digamma(0.5\tau_{jk}) \right] \notag\\
        &+ \zv_n (1-\gammav_{jk}) \left[ B_{jk} \digamma(Y_{njk}+B_{jk}\tau_{jk})
        + (1-B_{jk}) \digamma(T_{njk}-Y_{njk}+(1-B_{jk})\tau_{jk}) \right. \notag\\
        &- \left. \digamma(T_{njk}+\tau_{jk}) + \digamma(\tau_{jk})
        - B_{jk} \digamma(B_{jk}\tau_{jk}) - (1-B_{jk}) \digamma((1-B_{jk})\tau_{jk}) \right]
\end{align}

\section{Inference}
Inference on binding sites can be performed by using
\begin{align}
	\frac{\Pr(\text{binding site} | \text{data})}{\Pr(\text{not binding site} | \text{data})},
\end{align}
where
\begin{align}
	\Pr(\text{not binding site} | \text{data})
	 &= \Pr(Z_n = 0 | X_n, T^X_n) + \Pr(Z_n = 1, \gamma_{jk} = 1 \  \forall j,k  | X_n, T^X_n)\\
	\Pr(\text{binding site} | \text{data})
	&= 1- \Pr(\text{not binding site} | \text{data}).
\end{align}
Here, 
\begin{align}
	\Pr(Z_n = 1, \gamma_{jk} = 1 \  \forall j,k  | X_n, T^X_n)
	 &= \Pr(\gamma_{jk} = 1 \  \forall j,k  | Z_n = 1, X_n, T^X_n)\Pr(Z_n = 1 | X_n, T^X_n)\\
	 &= \prod_{jk}[\Pr(\gamma_{jk} = 1 | Z_n = 1, X_n, T^X_n)]\Pr(Z_n = 1 | X_n, T^X_n).
\end{align}

 
\end{document}
